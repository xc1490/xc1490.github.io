<!DOCTYPE html>
<html>
    <head>
        <meta charset="UTF-8">
        <title>Neural Speech Decoding</title>
        <link rel="stylesheet" type="text/css" href="styles.css">
        <script src="jquery-3.5.js"></script>
    </head>
    <body>
<div class="container">
    <div id="text1">Subject-Agnostic Transformer-Based Neural Speech Decoding from Surface and Depth Electrode Signals</div>
    <div id="intro">
        
        
    </div>
</div>
        <div class="content-container">
            <div style="text-align:center; white-space: nowrap;">

                <a href="https://pubmed.ncbi.nlm.nih.gov/38559163/" style="color:blue; text-decoration:underline; display:inline-block; vertical-align:middle;">
                    [PAPER LINK]
                </a>

                </p>
            </div>
            
    


            <br/>
            <br/>
            <div style="text-align: center; border: 2px solid black; padding: 10px; display: inline-block;">
                <img src="fig2.png" style="width">
                <p style="font-style: italic;  font-size:medium">  <strong>Figure 1</strong>:Multiple-subject Neural Decoder training pipeline. Each participant’s neural signal and
                    electrodes’ location information (MNI coordinates and ROI index) are fed to a shared SwinTW
                    Neural Decoder to predict speech parameters. The predicted speech parameters are supervised
                    by the speech parameters generated by the subject-specific Speech Encoder from the ground-truth
                    speech spectrogram. Each participant’s predicted speech parameters are fed into the corresponding
                    subject-specific Speech Synthesizer to generate a speech spectrogram. Once the shared SwinTW
                    decoder is trained, it can be used to decode the speech from the neural signal of any participant.
                    The SwinTW decoder takes the neural signals and location information of the electrodes of this
                    participant as the input and generates speech parameters. The speech synthesizer (pre-trained
                    using the speech signal of the participant) then converts the generated speech parameters to the
                    decoded spectrogram. Note that the same training pipeline can be used to train a subject-specific
                    model using data from a single participant. The inference pipeline is shown in <b>b</b>. SwinTW in <b>c</b>
                    uses three stages of transformer blocks with spatial-temporal attention with temporal windowing
                    to extract features across multiple subjects. Transposed temporal convolution is used to upsample
                    the temporal dimension to be the same as the input. A prediction head module generates speech
                    parameters from the latent representation. </p>
            </div>
            <p>


<strong>Objective</strong>: This study investigates speech decoding from neural signals captured by intracranial electrodes. Most prior works can only work with electrodes on a 2D grid (i.e., Electrocorticographic or ECoG array) and data from a single patient. We aim to design a deep-learning model architecture that can accommodate both surface (ECoG) and depth (stereotactic EEG or sEEG) electrodes. The architecture should allow training on data from multiple participants with large variability in electrode placements and the trained model should perform well on participants unseen during training.
<br/>
<strong>Approach</strong>: We propose a novel transformer-based model architecture named SwinTW that can work with arbitrarily positioned electrodes, by leveraging their 3D locations on the cortex rather than their positions on a 2D grid. We train both subject- specific models using data from a single participant as well as multi-patient models exploiting data from multiple participants.
<br/>
<strong>Main Results</strong>: The subject-specific models using only low-density 8x8 ECoG data achieved high decoding Pearson Correlation Coefficient with ground truth spectrogram (PCC=0.817), over N=43 participants, outperforming our prior convolutional ResNet model and the 3D Swin transformer model. Incorporating additional strip, depth, and grid electrodes available in each participant (N=39) led to further improvement (PCC=0.838). For participants with only sEEG electrodes (N=9), subject-specific models still enjoy comparable performance with an average PCC=0.798. The multi- subject models achieved high performance on unseen participants, with an average PCC=0.765 in leave-one-out cross-validation.
<br/>
<strong>Significance</strong>: The proposed SwinTW decoder enables future speech neuropros- theses to utilize any electrode placement that is clinically optimal or feasible for a particular participant, including using only depth electrodes, which are more routinely implanted in chronic neurosurgical procedures. Importantly, the generalizability of the multi-patient models suggests the exciting possibility of developing speech neuropros- theses for people with speech disability without relying on their own neural data for training, which is not always feasible.
            </p>
            <br/>
        </div>
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

<div class="content-container">

    <!-- <div style="display: flex; justify-content: center; align-items: center; gap: 20px;">
        <div style="border: 2px solid black; padding: 10px; text-align: center;">
            <video width="90%" controls>
                <source src="video/LD_swint_s1_1s_s1.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <p style="font-style: italic; margin: 0;">Decoded Audio Demo (Decoded+Original)</p>
        </div>
    
        <div style="border: 2px solid black; padding: 10px; text-align: center;">
            <video width="90%" controls>
                <source src="video/LD_swint_s1_1s_s2.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <p style="font-style: italic; margin: 0;">Decoded Audio Demo (Original+Decoded)</p>
        </div>
    </div> -->
 
    <br/>
    <br/>

    <table>
        <tr>
            <th></th>
            <th>Single Subject</th>
            <th>Multi-Subject</th>
            <th>Multi-Subject on Unseen</th>
        </tr>
        <tr>
            <th>ECoG</th>
            <td>
                <audio controls>
                    <source src="audio/ecog_single_subject.wav" type="audio/mpeg">
                    Your browser does not support the audio element.
                </audio>
            </td>
            <td>
                <audio controls>
                    <source src="audio/ecog_multi_subject.wav" type="audio/mpeg">
                    Your browser does not support the audio element.
                </audio>
            </td>
            <td>
                <audio controls>
                    <source src="audio/ecog_multi_subject_unseen.wav" type="audio/mpeg">
                    Your browser does not support the audio element.
                </audio>
            </td>
        </tr>
        <tr>
            <th>sEEG</th>
            <td>
                <audio controls>
                    <source src="audio/seeg_single_subject.wav" type="audio/mpeg">
                    Your browser does not support the audio element.
                </audio>
            </td>
            <td>
                <audio controls>
                    <source src="audio/seeg_multi_subject.wav" type="audio/mpeg">
                    Your browser does not support the audio element.
                </audio>
            </td>
            <td>
                <audio controls>
                    <source src="audio/seeg_multi_subject_unseen.wav" type="audio/mpeg">
                    Your browser does not support the audio element.
                </audio>
            </td>
        </tr>
        <tr></tr>
            <th>bad examples</th>
            <td>
                <audio controls>
                    <source src="audio/ecog_bad1.wav" type="audio/mpeg">
                    Your browser does not support the audio element.
                </audio>
            </td>
            <td>
                <audio controls>
                    <source src="audio/ecog_bad2.wav" type="audio/mpeg">
                    Your browser does not support the audio element.
                </audio>
            </td>
            <td>
                <audio controls>
                    <source src="audio/ecog_bad3.wav" type="audio/mpeg">
                    Your browser does not support the audio element.
                </audio>
            </td>
        </tr>
    </table>


    <a   style="color:blue; text-decoration:none; display:inline-block; vertical-align:middle;">
        CITE OUR WORK
    </a>
    
        <div style="background-color: #f2f2f2; padding: 20px; border-radius: 8px; font-family: 'Arial', sans-serif; font-size: 14px; color: #333; margin: 20px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
        <pre style="white-space: pre-wrap; word-wrap: break-word;">
@article{chen2024subject,
    title={Subject-Agnostic Transformer-Based Neural Speech Decoding from Surface and Depth Electrode Signals},
    author={Chen, Junbo and Chen, Xupeng and Wang, Ran and Le, Chenqian and Khalilian-Gourtani, Amirhossein and Jensen, Erika and Dugan, Patricia and Doyle, Werner and Devinsky, Orrin and Friedman, Daniel and others},
    journal={bioRxiv},
    year={2024},
    publisher={Cold Spring Harbor Laboratory Preprints}
    }
        </pre>
      </div>
      
</div>





</body>
</html>
