<!DOCTYPE html>
<html>
    <head>
        <meta charset="UTF-8">
        <title>Neural Speech Decoding</title>
        <link rel="stylesheet" type="text/css" href="styles.css">
        <script src="jquery-3.5.js"></script>
    </head>
    <body>
<div class="container">
    <div id="text1">Subject-Agnostic Transformer-Based Neural Speech Decoding from Surface and Depth Electrode Signals</div>
    <div id="intro">
        
        
    </div>
</div>
        <div class="content-container">
            <div style="text-align:center; white-space: nowrap;">

                <a href="https://pubmed.ncbi.nlm.nih.gov/38559163/" style="color:blue; text-decoration:underline; display:inline-block; vertical-align:middle;">
                    [PAPER LINK]
                </a>

                </p>
            </div>
            
    

            <div style="text-align: center; border: 2px solid black; padding: 10px; display: inline-block;">
                <img src="fig1.png" style="width">
                <p style="font-style: italic; font-size:medium"> <strong>Figure 1</strong>: Speech Decoding Training Pipeline. SwinTW uses three stages of
                    transformer blocks with spatial-temporal attention with temporal windowing to extract features.
                    An MLP layer is applied to decrease the latent dimension after patch merging. After the last
                    transformer block, spatial max pooling is applied across the electrodes to generate a single feature
                    per time step. Finally, transposed temporal convolution is used to upsample the temporal dimension
                    to be the same as the input.</p>
            </div>
            <br/>
            <br/>
            <div style="text-align: center; border: 2px solid black; padding: 10px; display: inline-block;">
                <img src="fig2.png" style="width">
                <p style="font-style: italic;  font-size:medium">  <strong>Figure 2</strong>: Multiple-subject Neural Decoder training pipeline. Each participant’s
                    neural signal and electrodes’ location information (MNI coordinates and ROI index) are fed to a
                    shared SwinTW Neural Decoder to predict speech parameters. The predicted speech parameters
                    are supervised by the speech parameters generated by the subject-specific Speech Encoder from the
                    ground-truth speech spectrogram. Each participant’s predicted speech parameters are fed into the
                    corresponding subject-specific Speech Synthesizer to generate a speech spectrogram.</p>
            </div>
            <p>


<strong>Objective</strong>: This study investigates speech decoding from neural signals captured by intracranial electrodes. Most prior works can only work with electrodes on a 2D grid (i.e., Electrocorticographic or ECoG array) and data from a single patient. We aim to design a deep-learning model architecture that can accommodate both surface (ECoG) and depth (stereotactic EEG or sEEG) electrodes. The architecture should allow training on data from multiple participants with large variability in electrode placements and the trained model should perform well on participants unseen during training.
<br/>
<strong>Approach</strong>: We propose a novel transformer-based model architecture named SwinTW that can work with arbitrarily positioned electrodes, by leveraging their 3D locations on the cortex rather than their positions on a 2D grid. We train both subject- specific models using data from a single participant as well as multi-patient models exploiting data from multiple participants.
<br/>
<strong>Main Results</strong>: The subject-specific models using only low-density 8x8 ECoG data achieved high decoding Pearson Correlation Coefficient with ground truth spectrogram (PCC=0.817), over N=43 participants, outperforming our prior convolutional ResNet model and the 3D Swin transformer model. Incorporating additional strip, depth, and grid electrodes available in each participant (N=39) led to further improvement (PCC=0.838). For participants with only sEEG electrodes (N=9), subject-specific models still enjoy comparable performance with an average PCC=0.798. The multi- subject models achieved high performance on unseen participants, with an average PCC=0.765 in leave-one-out cross-validation.
<br/>
<strong>Significance</strong>: The proposed SwinTW decoder enables future speech neuropros- theses to utilize any electrode placement that is clinically optimal or feasible for a particular participant, including using only depth electrodes, which are more routinely implanted in chronic neurosurgical procedures. Importantly, the generalizability of the multi-patient models suggests the exciting possibility of developing speech neuropros- theses for people with speech disability without relying on their own neural data for training, which is not always feasible.
            </p>
            <br/>
        </div>
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

<div class="content-container">

    <br/>
        <br/>
        Some good decoding results:
        <br/>
        <br/>

    <div style="display: flex; justify-content: center; align-items: center; gap: 20px;">
        <div style="border: 2px solid black; padding: 10px; text-align: center;">
            <video width="90%" controls>
                <source src="video/LD_swint_s1_1s_s1.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <p style="font-style: italic; margin: 0;">Decoded Audio Demo (Decoded+Original)</p>
        </div>
    
        <div style="border: 2px solid black; padding: 10px; text-align: center;">
            <video width="90%" controls>
                <source src="video/LD_swint_s1_1s_s2.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <p style="font-style: italic; margin: 0;">Decoded Audio Demo (Original+Decoded)</p>
        </div>
    </div>
 
    <br/>
    <br/>

    Note that the decoded speech quality is not always perfect. Here is an example of a bad decoding result:
    <br/>
    <br/>
    <audio controls>
        <source src="audio/bad_demo.wav" type="audio/mpeg">
        Your browser does not support the audio element.
      </audio>

      <br/>
      <br/>
    <a   style="color:blue; text-decoration:none; display:inline-block; vertical-align:middle;">
        CITE OUR WORK
    </a>
    
        <div style="background-color: #f2f2f2; padding: 20px; border-radius: 8px; font-family: 'Arial', sans-serif; font-size: 14px; color: #333; margin: 20px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
        <pre style="white-space: pre-wrap; word-wrap: break-word;">
@article{chen2024subject,
    title={Subject-Agnostic Transformer-Based Neural Speech Decoding from Surface and Depth Electrode Signals},
    author={Chen, Junbo and Chen, Xupeng and Wang, Ran and Le, Chenqian and Khalilian-Gourtani, Amirhossein and Jensen, Erika and Dugan, Patricia and Doyle, Werner and Devinsky, Orrin and Friedman, Daniel and others},
    journal={bioRxiv},
    year={2024},
    publisher={Cold Spring Harbor Laboratory Preprints}
    }
        </pre>
      </div>
      
</div>





</body>
</html>
