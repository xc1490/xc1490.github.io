<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Neural Speech Decoding</title>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&family=JetBrains+Mono&display=swap">
        <link rel="stylesheet" type="text/css" href="styles.css">
        <script src="jquery-3.5.js"></script>
    </head>
    <body>
        <main class="container">
            <header>
                <h1 id="text1">Transformer-Based Neural Speech Decoding from Surface and Depth Electrode Signals</h1>
                <div style="text-align:center;">
                    <a href="https://iopscience.iop.org/article/10.1088/1741-2552/adab21" class="paper-link">
                        VIEW PAPER
                    </a>
                </div>
            </header>

            <div class="content-container">
                <div class="figure-container">
                    <img src="fig2.png" alt="Neural Decoder Training Pipeline" loading="lazy">
                    <p class="figure-caption"><strong>Figure 1</strong>: Multiple-subject Neural Decoder training pipeline. Each participant's neural signal and electrodes' location information (MNI coordinates and ROI index) are fed to a shared SwinTW Neural Decoder to predict speech parameters. The predicted speech parameters are supervised by the speech parameters generated by the subject-specific Speech Encoder from the ground-truth speech spectrogram. Each participant's predicted speech parameters are fed into the corresponding subject-specific Speech Synthesizer to generate a speech spectrogram. Once the shared SwinTW decoder is trained, it can be used to decode the speech from the neural signal of any participant. The SwinTW decoder takes the neural signals and location information of the electrodes of this participant as the input and generates speech parameters. The speech synthesizer (pre-trained using the speech signal of the participant) then converts the generated speech parameters to the decoded spectrogram. Note that the same training pipeline can be used to train a subject-specific model using data from a single participant. The inference pipeline is shown in <b>b</b>. SwinTW in <b>c</b> uses three stages of transformer blocks with spatial-temporal attention with temporal windowing to extract features across multiple subjects. Transposed temporal convolution is used to upsample the temporal dimension to be the same as the input. A prediction head module generates speech parameters from the latent representation.</p>
                </div>

                <section>
                    <h2 class="section-title">Objective</h2>
                    <p>This study investigates speech decoding from neural signals captured by intracranial electrodes. Most prior works can only work with electrodes on a 2D grid (i.e., Electrocorticographic or ECoG array) and data from a single patient. We aim to design a deep-learning model architecture that can accommodate both surface (ECoG) and depth (stereotactic EEG or sEEG) electrodes. The architecture should allow training on data from multiple participants with large variability in electrode placements and the trained model should perform well on participants unseen during training.</p>
                </section>

                <section>
                    <h2 class="section-title">Approach</h2>
                    <p>We propose a novel transformer-based model architecture named SwinTW that can work with arbitrarily positioned electrodes, by leveraging their 3D locations on the cortex rather than their positions on a 2D grid. We train both subject-specific models using data from a single participant as well as multi-patient models exploiting data from multiple participants.</p>
                </section>

                <section>
                    <h2 class="section-title">Main Results</h2>
                    <p>The subject-specific models using only low-density 8x8 ECoG data achieved high decoding Pearson Correlation Coefficient with ground truth spectrogram (PCC=0.817), over N=43 participants, outperforming our prior convolutional ResNet model and the 3D Swin transformer model. Incorporating additional strip, depth, and grid electrodes available in each participant (N=39) led to further improvement (PCC=0.838). For participants with only sEEG electrodes (N=9), subject-specific models still enjoy comparable performance with an average PCC=0.798. The multi-subject models achieved high performance on unseen participants, with an average PCC=0.765 in leave-one-out cross-validation.</p>
                </section>

                <section>
                    <h2 class="section-title">Significance</h2>
                    <p>The proposed SwinTW decoder enables future speech neuroprostheses to utilize any electrode placement that is clinically optimal or feasible for a particular participant, including using only depth electrodes, which are more routinely implanted in chronic neurosurgical procedures. Importantly, the generalizability of the multi-patient models suggests the exciting possibility of developing speech neuroprostheses for people with speech disability without relying on their own neural data for training, which is not always feasible.</p>
                </section>

                <section>
                    <h2 class="section-title">Audio Demonstrations</h2>
                    <div class="audio-demos">
                        <table class="audio-table">
                            <thead>
                                <tr>
                                    <th></th>
                                    <th>Single Subject</th>
                                    <th>Multi-Subject<sup>1</sup></th>
                                    <th>Multi-Subject on Unseen<sup>2</sup></th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <th>ECoG</th>
                                    <td>
                                        <audio controls>
                                            <source src="audio/ecog_single_subject.wav" type="audio/mpeg">
                                            Your browser does not support the audio element.
                                        </audio>
                                    </td>
                                    <td>
                                        <audio controls>
                                            <source src="audio/ecog_multi_subject.wav" type="audio/mpeg">
                                            Your browser does not support the audio element.
                                        </audio>
                                    </td>
                                    <td>
                                        <audio controls>
                                            <source src="audio/ecog_multi_subject_unseen.wav" type="audio/mpeg">
                                            Your browser does not support the audio element.
                                        </audio>
                                    </td>
                                </tr>
                                <tr>
                                    <th>sEEG</th>
                                    <td>
                                        <audio controls>
                                            <source src="audio/seeg_single_subject.wav" type="audio/mpeg">
                                            Your browser does not support the audio element.
                                        </audio>
                                    </td>
                                    <td>
                                        <audio controls>
                                            <source src="audio/seeg_multi_subject.wav" type="audio/mpeg">
                                            Your browser does not support the audio element.
                                        </audio>
                                    </td>
                                    <td>
                                        <audio controls>
                                            <source src="audio/seeg_multi_subject_unseen.wav" type="audio/mpeg">
                                            Your browser does not support the audio element.
                                        </audio>
                                    </td>
                                </tr>
                                <tr>
                                    <th>Some Failure Cases</th>
                                    <td>
                                        <audio controls>
                                            <source src="audio/ecog_bad1.wav" type="audio/mpeg">
                                            Your browser does not support the audio element.
                                        </audio>
                                    </td>
                                    <td>
                                        <audio controls>
                                            <source src="audio/ecog_bad2.wav" type="audio/mpeg">
                                            Your browser does not support the audio element.
                                        </audio>
                                    </td>
                                    <td>
                                        <audio controls>
                                            <source src="audio/ecog_bad3.wav" type="audio/mpeg">
                                            Your browser does not support the audio element.
                                        </audio>
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                        <p class="footnote"><sup>1</sup> Multi-Subject: Training a multi-subject model using N subjects' training set and test on one of the N subjects on its testset</p>
                        <p class="footnote"><sup>2</sup> Multi-Subject on Unseen: Training a multi-subject model using N subjects' training set and test on an unseen subject on its testset</p>
                    </div>
                </section>

                <section>
                    <h2 class="section-title">Citation</h2>
                    <div class="citation-box">
                        <button class="copy-button" onclick="copyCitation()">
                            <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
                            </svg>
                            Copy
                        </button>
                        <pre id="citation">@article{chen2025transformer,
    title={Transformer-based neural speech decoding from surface and depth electrode signals},
    author={Chen, Junbo and Chen, Xupeng and Wang, Ran and Le, Chenqian and Khalilian-Gourtani, Amirhossein and Jensen, Erika and Dugan, Patricia and Doyle, Werner and Devinsky, Orrin and Friedman, Daniel and others},
    journal={Journal of Neural Engineering},
    volume={22},
    number={1},
    pages={016017},
    year={2025},
    publisher={IOP Publishing}
}</pre>
                    </div>
                </section>
            </div>
        </main>

        <script>
        function copyCitation() {
            const citation = document.getElementById('citation').textContent;
            navigator.clipboard.writeText(citation).then(() => {
                const button = document.querySelector('.copy-button');
                button.innerHTML = `
                    <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7" />
                    </svg>
                    Copied!
                `;
                button.classList.add('copied');
                
                setTimeout(() => {
                    button.innerHTML = `
                        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
                        </svg>
                        Copy
                    `;
                    button.classList.remove('copied');
                }, 2000);
            });
        }
        </script>
    </body>
</html>
