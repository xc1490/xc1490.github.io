<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Neural Speech Decoding</title>
        <link rel="stylesheet" type="text/css" href="styles.css">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    </head>
    <body>
        <div class="container">
            <header>
                <h1 id="text1">Decoding Imagined and Overt Speech from sEEG Neural Recordings</h1>
                <div class="subtitle">Advancing Neural Speech Processing Technology</div>
            </header>

            <div class="content-container">
                <div class="hero-section">
                    <img src="fig1.png" alt="Neural Speech Decoding Diagram" class="hero-image">
                </div>
                


                <div class="visualization-section">
                    <img src="fig1.png" alt="Neural Speech Decoding Diagram" class="result-image">
                    <div class="figure-caption">
                        <p class="caption-title">Neural Speech Decoding Framework with HuBERT Synthesizer</p>
                        <p class="caption-text">
                            The proposed neural speech decoding framework consists of two main components:
                        </p>
                        <div class="caption-section">
                            <p class="caption-text"><strong>Upper part:</strong> The sEEG-to-speech decoding pipeline. The sEEG decoder generates time-varying logits from sEEG signals, which are converted into units. A pre-trained Tacontron2 model takes the decoded units and generates a mel spectrogram, which is used by Waveglow to decode the speech.</p>
                        </div>
                        <div class="caption-section">
                            <p class="caption-text"><strong>Lower part:</strong> The pre-trained HuBERT speech synthesizer takes speech as input and generates features that are used for clustering to generate quantized units per frame. The pre-trained Tacontron2 model takes the quantized units and generates a mel spectrogram, which is used by Waveglow to resynthesize the speech.</p>
                        </div>
                        <p class="caption-text caption-note">
                            The lower speech re-synthesis part serves as a guide to generate quantized units, helping the sEEG decoder map to the units as closely as possible. The sEEG decoder functions as a classifier that generates time-varying class labels for each frame. During inference, only the upper part is used.
                        </p>
                    </div>
                </div>

                <div class="visualization-section">
                    <img src="fig2.png" alt="Research Results Visualization" class="result-image">
                </div>

                <div class="visualization-section">
                    <img src="art.png" alt="Neural Speech Decoding Framework with Articulatory Space" class="result-image">
                    <div class="figure-caption">
                        <p class="caption-title">Neural Speech Decoding Framework with Articulatory Space</p>
                        <p class="caption-text">
                            <strong>Upper panel:</strong> sEEG‐to‐speech decoding. Intracranial sEEG signals recorded during auditory/visual tasks are mapped by an sEEG decoder into time‐varying latent vectors (12‐D articulatory trajectories + pitch + loudness), which—together with a learned speaker embedding—condition a pretrained HiFi‐GAN vocoder to produce decoded speech.
                        </p>
                        <p class="caption-text">
                            <strong>Lower panel:</strong> Articulatory analysis–synthesis for training. Original speech is processed by separate speaker identity, articulatory feature, and source feature extractors to generate reference articulatory kinematics and source signals. These reference features drive the same HiFi‐GAN to re‐synthesize speech, and the reconstructed waveform provides supervisory feedback (via MSE/Huber loss) to align the sEEG‐decoded latents. During inference only the upper pathway is used.
                        </p>
                    </div>
                </div>
            </div>

            <div class="content-container">
                <div class="content-title">Speech Decoding Examples</div>
                <div class="audio-demos">
                    <h3 class="section-subtitle">Results from HuBERT Synthesizer Framework</h3>
                    <div class="demo-image-container">
                        <img src="demo_hubert.png" alt="HuBERT Synthesis Demo" class="demo-image">
                    </div>
                    <table class="inlineTable">
                        <colgroup>
                            <col style="width: 20%">
                            <col style="width: 80%">
                        </colgroup>
                        <tr>
                            <td></td>
                            <td><strong>sEEG Overt</strong></td>
                        </tr>
                        <tr>
                            <th>Decoded</th>
                            <td class="audio-cell">
                                <audio controls>
                                    <source src="./audio/PRED/overt.wav" type="audio/wav">
                                    Your browser does not support the audio element.
                                </audio>
                            </td>
                        </tr>
                        <tr>
                            <th>Original/Decoded</th>
                            <td class="audio-cell">
                                <audio controls>
                                    <source src="./audio/MERGE/overt.wav" type="audio/wav">
                                    Your browser does not support the audio element.
                                </audio>
                            </td>
                        </tr>
                    </table>

                    <h3 class="section-subtitle">Results from Articulatory Space Decoding</h3>
                    <div class="demo-image-container">
                        <img src="demo_art.png" alt="Articulatory Space Decoding Demo" class="demo-image">
                    </div>
                    <table class="inlineTable">
                        <colgroup>
                            <col style="width: 20%">
                            <col style="width: 80%">
                        </colgroup>
                        <tr>
                            <td></td>
                            <td><strong>sEEG Overt</strong></td>
                        </tr>
                        <tr>
                            <th>Decoded</th>
                            <td class="audio-cell">
                                <audio controls>
                                    <source src="decoded.wav" type="audio/wav">
                                    Your browser does not support the audio element.
                                </audio>
                            </td>
                        </tr>
                        <tr>
                            <th>Original/Decoded</th>
                            <td class="audio-cell">
                                <audio controls>
                                    <source src="merged.wav" type="audio/wav">
                                    Your browser does not support the audio element.
                                </audio>
                            </td>
                        </tr>
                    </table>
                </div>
            </div>
        </div>
    </body>
</html>
